{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from groundingdino.util.inference import Model\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "import config.config as cfg\n",
    "import params\n",
    "import utils\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905971214/work/aten/src/ATen/native/TensorShape.cpp:3587.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# Building GroundingDINO inference model\n",
    "grounding_dino_model = Model(\n",
    "    model_config_path=cfg.GROUNDING_DINO_CONFIG_PATH,\n",
    "    model_checkpoint_path=cfg.GROUNDING_DINO_CHECKPOINT_PATH,\n",
    ")\n",
    "\n",
    "# Building SAM Model and SAM Predictor\n",
    "sam = sam_model_registry[cfg.SAM_ENCODER_VERSION](checkpoint=cfg.SAM_CHECKPOINT_PATH)\n",
    "sam.to(device=params.DEVICE)\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for storing segmented lizards, images, and labels\n",
    "os.makedirs(cfg.SEGMENTED_LIZARDS_PATH, exist_ok=True)\n",
    "os.makedirs(cfg.SEGMENTED_IMAGES_PATH, exist_ok=True)\n",
    "os.makedirs(cfg.SEGMENTED_LABELS_PATH, exist_ok=True)\n",
    "\n",
    "# Iterate over each island directory\n",
    "for island_path in glob.glob(f\"{cfg.RAW_IMAGES_PATH}/*\"):\n",
    "    print(os.path.splitext(os.path.basename(island_path))[0])\n",
    "\n",
    "    # Randomly select 100 lizards to segment their body parts\n",
    "    for lizard_path in np.random.choice(glob.glob(f\"{island_path}/*\"), 100):\n",
    "\n",
    "        # Process each image of the lizard\n",
    "        for image_path in glob.glob(f\"{lizard_path}/*.jpg\"):\n",
    "            # Clear CUDA cache to free memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Get image name without extension\n",
    "            image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            image = cv2.imread(image_path)\n",
    "            gsam_image_path = f\"{cfg.SEGMENTED_LIZARDS_PATH}/gs_{image_name}.jpg\"\n",
    "\n",
    "            image_output_path = f\"{cfg.SEGMENTED_IMAGES_PATH}/{image_name}.jpg\"\n",
    "            label_output_path = f\"{cfg.SEGMENTED_LABELS_PATH}/{image_name}.txt\"\n",
    "\n",
    "            # Skip processing if the image has already been processed\n",
    "            if os.path.exists(gsam_image_path):\n",
    "                print(f\"Image {image_name} already processed\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing image: {image_name}\")\n",
    "\n",
    "            # Detect objects using GroundingDINO\n",
    "            detections = grounding_dino_model.predict_with_classes(\n",
    "                image=image,\n",
    "                classes=params.CLASSES,\n",
    "                box_threshold=params.BOX_THRESHOLD,\n",
    "                text_threshold=params.TEXT_THRESHOLD,\n",
    "            )\n",
    "\n",
    "            # Apply Non-Maximum Suppression (NMS) to filter detections\n",
    "            nms_idx = (\n",
    "                torchvision.ops.nms(\n",
    "                    torch.from_numpy(detections.xyxy),\n",
    "                    torch.from_numpy(detections.confidence),\n",
    "                    params.NMS_THRESHOLD,\n",
    "                )\n",
    "                .numpy()\n",
    "                .tolist()\n",
    "            )\n",
    "\n",
    "            # Update detections after NMS\n",
    "            detections.xyxy = detections.xyxy[nms_idx]\n",
    "            detections.confidence = detections.confidence[nms_idx]\n",
    "            detections.class_id = detections.class_id[nms_idx]\n",
    "\n",
    "            try:\n",
    "                # Select the detection with the highest confidence\n",
    "                max_confidence_idx = np.argmax(detections.confidence)\n",
    "                detections.xyxy = detections.xyxy[max_confidence_idx].reshape(1, 4)\n",
    "                detections.confidence = detections.confidence[\n",
    "                    max_confidence_idx\n",
    "                ].reshape(1)\n",
    "                detections.class_id = detections.class_id[max_confidence_idx].reshape(1)\n",
    "\n",
    "                # Convert detections to masks using SAM predictor\n",
    "                detections.mask = utils.segment(\n",
    "                    sam_predictor=sam_predictor,\n",
    "                    image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
    "                    xyxy=detections.xyxy,\n",
    "                )\n",
    "\n",
    "                # Annotate image with detections\n",
    "                annotated_image = utils.annotateImageWithDetections(image, detections)\n",
    "\n",
    "                # Extract the first mask (assuming it's the one we want)\n",
    "                mask = detections.mask[0]\n",
    "\n",
    "                # Convert mask to polygon\n",
    "                polygon = utils.mask_to_polygon(mask)\n",
    "\n",
    "                # Save the polygon as YOLO txt label and the annotated image\n",
    "                utils.save_polygon_label_as_yolo_txt(label_output_path, polygon)\n",
    "                cv2.imwrite(gsam_image_path, annotated_image)\n",
    "                cv2.imwrite(image_output_path, image)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image: {image_name}, Error: {str(e)}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is done by hand. We manually select the images that are perfectly segmented, which will be those that will be used to train the YOLOv8 instace segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, move the images and their corresponding labels to the train folder\n",
    "# if they are in the selected folder\n",
    "\n",
    "os.makedirs(f\"{cfg.IMAGES_PATH}\", exist_ok=True)\n",
    "os.makedirs(f\"{cfg.LABELS_PATH}\", exist_ok=True)\n",
    "\n",
    "for image_path in glob.glob(f\"{cfg.SELECTED_IMAGES_PATH}/*.jpg\"):\n",
    "    image_name = os.path.splitext(os.path.basename(image_path))[0][3:]\n",
    "    image_path = f\"{cfg.SEGMENTED_IMAGES_PATH}/{image_name}.jpg\"\n",
    "    label_path = f\"{cfg.SEGMENTED_LABELS_PATH}/{image_name}.txt\"\n",
    "\n",
    "    shutil.move(image_path, cfg.IMAGES_PATH)\n",
    "    shutil.move(label_path, cfg.LABELS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the selected images and their correesponding labels, we upload it to ROBOFLOW to be able to apply some preprocessing techniques more easily. In the next notebook, we train the segmentation model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lizard-segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
